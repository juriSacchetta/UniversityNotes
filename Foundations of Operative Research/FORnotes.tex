\documentclass[a4paper]{book}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{mwe}
\usepackage{float}
\usepackage{array}
\usepackage{soul}
\usepackage{multicol}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{property}{Property}


\begin{document}
	\author{Elia Ravella, Juri Sacchetta}
	\title{Foundations of Operations Research}
	\maketitle
	\tableofcontents
	\input{graphs.tex}
	\input{linear_programming.tex}
\end{document}


% \begin{document}
% 	\begin{titlepage}
% 		\maketitle
% 	\end{titlepage}
	
% 	\tableofcontents
% 	\clearpage

% 	\section{Graphs}
% 		\subsection{Definitions}
% 			A graph is a pair of sets. Graph $G = (N, E)$ is composed of a set N of nodes and a set E of edges, that are themselves pair of nodes. The intuitive representation is the classic web of interconnected nodes.\\
% 			Two nodes are said to be \textit{adjacent} if there's an edge connecting them.\\
% 			An edge is \textit{incident} to a node if it has that node at an endpoint.\\
% 			The \textit{degree} of a node is the number of incident edges to it.\\
% 			A \textit{path} is a subset of the E set composed of consecutive edges (the endpoint of an edge is the "start"point of the next one\footnote{not in a directional way: we are still talking about non directed graphs, the two consecutive edges just in fact share an endpoint}) that connects some edges in the graph.\\
% 			Two nodes are \textit{connected} if exists a path that links them. A graph con be \textit{connected} to, if and only if there exists a path the connects \textbf{all its nodes}.\\
% 			A graph is \textit{directed} if some of the edges can be traversed in only one way. These edges are called \textit{arcs}.\\
% 			A \textit{cycle} or \textit{circuit} is a directed path that end where it starts.\\
% 			A graph is \textit{complete} if for all pair of nodes there's an edge connecting them. Reasoning on this line, we can define the upper bounds for the number of edges given the nodes:
% 			\begin{equation}
% 				\begin{cases}
% 					e \leq \frac{n \times (n-1)}{2} \text{ if undirected} \\
% 					e \leq n \times (n-1) \text{ if directed}
% 				\end{cases}
% 			\end{equation}
% 			where intuitively "e" is the number of edges and "n" the number of nodes. Given this two inequalities, we can redefine completeness: a graph is complete if and only if $e = \frac{n \times (n - 1)}{2}$ or $e = n \times (n - 1)$ depending on the graph being directed.

% 			\paragraph{Cuts}
% 				Given an undirected graph $G = (N, E)$ we can take $S \subset G$ subsets of nodes; we then observe $\delta(S)$ the \textit{cut} induced by S is the subsets of \textbf{edges} that connects S to N \textbackslash S.
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/Cut.png}
% 				\end{figure}
% 				Same definition holds for directed graphs, but this time we can distinguish among \textit{outgoing} and \textit{incoming} cuts, wheter the edges are all exiting S or ending in it.\\
% 				A graph is said to be \textit{bipartite} if we find a partition $N = (N_1, N_2)$ such as $\delta(N_i) = \emptyset$. So \textbf{the two subsets are disconnected}.\\
% 				A bit of foreshadowing: if a cut from a partial tree to another portion of the graph holds a minimum cost arc, then \textit{there exists a spanning tree with that arc}. This is called the cut property.
				
% 			\paragraph{Data Representations for Graphs}
% 			Depending one the edges / nodes ratio, two different data structures can hold a valid representation for a graph. When $e \approx n^2$ (a dense graph) then an adjacency matrix is the best choice. Otherwise (sparse graphs) linked lists of successors for each node.  

% 		\subsection{Reachability}
% 			The reachability problem can be formulated as "given a graph, determine all the nodes that are reachable from a starting one, decided a priori. A node is reachable is there's a path from the starting one and itself".

% 		\subsection{Subgraphs, Trees, Spanning Trees}
% 			We call $G' = (N', E')$ a subgraph of $G = (N, E)$ if and only if
% 			\begin{equation}
% 				\begin{cases}
% 					N' \subseteq N \\
% 					E' \subseteq E \\
% 					\text{all edges inside $E'$ connects nodes in $N'$}
% 				\end{cases}
% 			\end{equation}
% 			A \textit{tree} $G_t = (N', E')$ is a subgraph of a network that's both \textbf{connected} and \textbf{acyclic}. A tree is \textit{spanning} if it contains all the nodes of the original graph. The \textit{leaves} of a tree are the nodes with unitary degree.

% 			Properties of trees:
% 			\begin{itemize}
% 				\item Every tree with a number of nodes greater than 2 has at least 2 leaves
% 				\item All trees have number of arcs equal to number of nodes minus 1
% 				\item Every pair of nodes in a tree is connected by a unique path (acyclic connected graph)
% 				\item By adding an edge to a tree, we create a \textit{unique cycle}. This also mean that if we now remove \textit{any} of the edges of this new unique cycle we go back to a spanning tree
% 				\item A complete Graph with n nodes has exactly $n^{n-2}$ spanning trees
% 			\end{itemize}

% 		\subsection{Minimum Cost Spanning Tree}
% 			Problem: find in a graph $G = (N, E)$ with \textit{weighted arcs}\footnote{numerated arcs with a "cost" associated to them} a subgraph that is a tree and has minimum total cost of edges. The "minimum total cost" condition is verified when the sum of the costs along edges cannot be decreased by swapping edges in and out from the tree.

% 			\paragraph{Prim's Algorithm}
% 				The Prim's algorithm is a simple iterative algorithm to build a spanning tree starting from an initial node.\\
% 				The algorithm adopts an incremental greedy strategy: it adds to the (initally empty) tree the nearest node every time (nearest = connected with the minimum cost outgoing edge) until the tree has the same number of nodes as the original graph.\\
% 				The full algorithm:
% 				\begin{enumerate}
% 					\item initialize the output: $S = (N', T)$ where $N'$ is composed only of the initial chosen node and $T = \emptyset$ is the set of edges
% 					\item add to S's nodes the "nearest" node from the ones connected to it, and add to T the lowest-cost edge. Careful: we are watching at \textit{all the nodes reachable from the outgoing cut of S}, not only the ones from a single node
% 					\item repeat step 2 until the graph is covered
% 				\end{enumerate}
% 				Prim's algorithm is exact\footnote{Formally proven to provide an optimal solution for each instance of a problem}.\\
% 				This algorithm exploits the \textbf{cut property}.

% 		\subsection{Shortest Path and Shortest Path Trees}
% 			Finding the shortest path on a graph from node A to node B is another classical problem. The most used algorithm is the famous Dijkstra's algorithm.

% 			\paragraph{Dijkstra Algorithm}
% 				Dijkstra algorithm is similar to the Prim's one, but applied in a different direction. Dijkstra starts from a subset of the nodes (called the "unvisited nodes") and considers themone at a time. At each iteration, a node is selected from the unvisited set (the first is chosen a priori, will be the \textit{initial} node) and for all his neighbours is updated the distance. When all neighbours have been inspected, the node is marked as "visited"\footnote{and never checked again}. Next node to be selected is the \textit{closest} and the algorithm restarts. It finishes when the "unvisited" set is empty. More schematically:
% 				\begin{enumerate}
% 					\item Select the closest node (first one a priori)
% 					\item Check his neighbours and update their distance
% 					\item When all neighbours have been inspected, go to point one. If no more unvisited nodes exists, end.
% 				\end{enumerate}
% 				At the end of the algorithm we have a \textit{shortest path tree}\footnote{that's \underline{\textbf{different}} from a minimum cost spanning tree} that highlights all the shortest paths from the initial node to all other nodes in the graph. Taking the \textit{closest} node at each iteration checking the outgoing cut from the "visited nodes" set ensures the exactness of Dijkstra algorithm.

% 			\paragraph{Floyd Warshall Algorithm}
% 				Negative-cost edges, if present, do not allow to apply Dijkstra algorithm. But they are a further threat to shortest path problems: if a \textit{negative cost cycle} exists, there's no minimum cost path that's also finite between two nodes that traverse that cycle. This is a ill-defined problem, and the Floyd Warshall algorithm detects it. This algorithm uses two matrixes: a \underline{distance} and a \underline{predecessor} one. At each iteration of the algorithm, a \textit{triangular check} is performed: for all his neighbours, an undirect path is searched (a multi step path, generally with a single intermediate node) to shorten the distance wrt the direct path.
% 				\begin{enumerate}
% 					\item initialize a counter \emph{h} at 1, it will be our index for the nodes
% 					\item for each value of the counter from 1 to the number of nodes, perform the check $d_{ij}\, >\, d_{ih}\, +\, d_{hj}$ to evaluate if there is an alternative two-steps path that reduces the distance between nodes $i$ and $j$.
% 					\item if a loop with negative cost is detected (so a node can go from itself to itself with less than 0 distance passing through another node) that the problem is flagged as ill and the algorithm terminates
% 				\end{enumerate}
% 				At the end, combining the two matrixes, we have a shortest path tree of the original graph. This time, it can be calculated with negative cost arches.

% 			\subsubsection{Direct Acyclic Graphs and Dynamic Programming}
% 				If we add the hypothesis that the graph we're working on (to find the shortes path or the minimum cost spanning tree) does not contain cycles, we can study some more interesting solutions to such a problem. Moreover, DAGs\footnote{directed acyclic graphs} are well suited to model a wide range of problems.

% 				\paragraph{Topological Ordering}
% 					DAGs can be \textit{topologically ordered}, which means that we can label the nodes as if the graph represents an order relation between them (in the algebraic sense). The formal definition is that
% 					\begin{equation}
% 						\forall (i, j) \in A\, |\, i\, <\, j
% 					\end{equation}
% 					where A is the set of edges. This looks like a trivial additional property to a graph, but having a \textit{strictly monotone ordering of nodes} is super helpful.\\
% 					To order nodes in such a way, however, the algorithm is quite simple:
% 					\begin{enumerate}
% 						\item find a node with no incoming edges\footnote{the existence of such a node is proved by the acyclicity condition} and assign the smallest index available
% 						\item remove the node from the graph and return to point 1
% 					\end{enumerate}
				
% 				\paragraph{Dynamic Programming}
% 					The "dynamic programming" approach has a bit of a confusing name. The underlying idea is that we can exploit a recursive relation between costs along a shortest path, using the "shortest subpath" property: given $\pi_{ij}$ shortest path $i \rightarrow j$ we can divide it in $\pi_{it} + c_{tj}$ where $\pi_{it}$ is the \textit{shortest subpath from i to t} and the second term is the cost of the last step. If we define $L(i)$ as the cost of the \underline{shortest path} from the initial node to node \emph{i} we have that
% 					\begin{equation}
% 						L(t) = min\{ L(i)_{i \text{ predecessor of } t}\, +\, c_{it} \}
% 					\end{equation}
% 					This is a recursive relation that can be expanded to all nodes in the path. I'll put here the example by the professor:
% 					\begin{figure}[H]
% 						\centering
% 						\includegraphics[width = \textwidth]{./images/DynProg.png}
% 					\end{figure}
% 					\begin{figure}[H]
% 						\centering
% 						\includegraphics[width = \textwidth]{./images/DynProg2.png}
% 					\end{figure}


% 			\subsubsection{Project Planning}
% 				As shown in the Examples, we can use graphs to express dependency relationships. A direct application of this model is used when organizing various activitie inside a project: each phase will depend from some previous phases, there will be an "initial" phase with no predecessors and a final phase with no successors. The model adopted here is the one that represents
% 				\begin{itemize}
% 					\item \textit{activities} as arcs
% 					\item \textit{activity duration} with arcs weight
% 					\item \textit{checkpoints} as nodes
% 				\end{itemize}
% 				This formulation leads to a weighted directed acyclic graph. It's acyclic \textit{by nature}: it would be meaningless (and also logically incorrect) if activities in a project had circular relationships.\\
% 				This model helps in finding the \textit{minimum overall project duration}: being directed and acyclic, this will be the duration of the \textit{longest path} from the initial node to the final (we cannot compress the project time beyond that).

% 				\paragraph{Critical Path Method}
% 					The CPM aims to provide an optimal schedule for a project, with allotted time for each activity and the possible slack of each one, given the DAG of a project activities precedences. The algorithm is:
% 					\begin{enumerate}
% 						\item Construct the DAG of activities and precedences
% 						\item Topologically sorts the nodes
% 						\item For each node, calculate
% 							\begin{itemize}
% 								\item The earliest start time starting from inital activity. This would be (at the final node) the minimum overall project duration
% 								\item The latest time an activity can be started to NOT increase the minimum project duration (starting from the end, this time)
% 							\end{itemize}
% 						\item For each activity now we can calculate the \textit{slack}: $T_{max} - T_{min} - d$ where \emph{d} is the duration of the activity itself
% 					\end{enumerate}
% 					The slack of each activity indicates if it could be \textit{deliberately delayed} without affecting the overall project duration. There exist activities with null slack: these are called \underline{critical activities} and cannot be delayed in any case. They compose the critical path from the beginning to the end of the project.
			
% 			\subsubsection{Network Flows}
% 				Network flows problems are the ones that involves "distribution of a given resource over a network". Needless to say, they boils down to find the optimal throughput of a network given its \textit{Directed Weighted Graph} representation. In this kind of problems, we can remove the acyclicity hypothesis. The graph model we're dealing with is like:
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/Flows.png}
% 					\caption{Arcs weights assume a different meaning here: they are not more a cost or a lenght, but instead a \textit{width} or \textit{capacity} as shown in the blue note}
% 				\end{figure}
% 				We introduce the \textit{feasible flow vector}: it's a tuple of values representing the \textbf{occupated fraction} of each edge of the network; it should respects two major constraints:
% 				\begin{enumerate}
% 					\item Capacity constraints: $\forall (i, j) \in E\, |\, (0\, \leq\, x_{ij}\, \leq\, k_{ij} )$ where $x_{ij}$ is the value of the flow in the feasible flow vector, $k_{ij}$ the maximum capacity for that arc and \emph{E} the set of edges;
% 					\item Flow balance constraints: $\forall h \in N\, |\, (\sum(x_{ih}) = \sum(x_{hj}))$ that simply states that for each node, the amount that enters the node is the same amount that exits it
% 				\end{enumerate}
% 				We can then calculate the \textit{value of flow} $\phi = \sum(x_{sj})$, where \emph{s} is the initial node and j all the nodes reached by the \textit{outgoing cut} of \emph{s}; this represents the actual maximum flow that can go through the network.\footnote{The maximum flow is directly associated with the minimum capacity of a cut. Will see later how these, in fact, are mathematically correlated: \textit{they're dual problems}, or two representations of the same problem.}

% 				\paragraph{Ford Fulkerson's Algorithm}
% 					Another algorithm, this time to resolve a network flow problem. Again, this is just the algorithmic version of the intuitive solution for this problem: keep sending stuff on non-saturated channels until the network is maxed out.\\
% 					Said so, we need some additional definition to handle easily this algorithm:
% 					\begin{itemize}
% 						\item a \textit{backward arc} is an edge of the network that "sends units backward" with regard to a simil-topological ordering of the nodes. We can "flat out" the graph of the network to visually understand what a backward arc is:
% 							\begin{figure}[H]
% 								\centering
% 								\includegraphics[width = \textwidth]{./images/BackwardArcs.png}
% 							\end{figure}
% 						\item an \textit{augmenting path} (wrt the current $\phi$) is a path through all the network (so start to end) where
% 							\begin{equation} 
% 								\begin{cases}
% 									x_{ij}\, <\, k_{ij} \text{ for all forward arcs}\\
% 									x_{ij}\, >\, 0 \text{ for all backward arcs}
% 								\end{cases}
% 							\end{equation}
% 						\item the \textit{residual network} of a graph is a graph itself buil keeping track of the possible changes that can be applied to the flows on the arcs. This means, if we have a \underline{saturated channel} with capacity 3 we will have in the residual network the same channel but \textit{reversed}: this signals that we have a residual capacity of 3 units \textit{in the opposite direction}.
% 							\begin{figure}[H]
% 								\centering
% 								\includegraphics[width = \textwidth]{./images/Residual.png}
% 							\end{figure}
% 					\end{itemize}
% 					So, the algorithm is structured in 2 main phases:
% 					\begin{enumerate}
% 						\item Search for an augmenting path on the actual graph, then rebuild the residual graph. Repeat this point until no more trivial augmenting path can be found
% 						\item Look for an augmenting path \textit{on the residual graph}. Using the second graph is useful when some edges could be desaturated in order to maximize the flow. The algorithm stops when on the residual graph there are no more paths from \emph{s} to \emph{t}
% 					\end{enumerate}



% 	\clearpage
% 	\section{Complexity}
% 		An \textit{algorithm} is a set of instructions that allows a machine to solve all instances of a given problem. Its executions time depends on the instance itself (usually its dimension) and the computer carrying out the operations. The complexity of an algorithm is the "number  of elementary operations\footnote{memory access, arithmetic ops, conrontations} needed to complete it" in the worst case (so big-O scenario / notation).\\
% 		Complexity is used to estabilish two kind of things:
% 		\begin{itemize}
% 			\item The complexity of a given algorithm to solve a given problem
% 			\item The inherent difficulty of a problem
% 		\end{itemize}

% 		\subsection{Problems and Instances}
% 			If we want to estimate the performance of \textit{alternative} algorithm on a given problem, we should not forget that performances are conditioned by the instance size in a significative way. The formal definition for "size of an instance" is the number of bits needed to encode it. Quick example (take a look at indices)
% 			\begin{equation}
% 				I = \begin{cases}
% 					m\\
% 					c_1 ... c_m\\
% 				\end{cases}
% 			\end{equation}
% 			both $m$ and $c_j$ are integers. The computation of the instance size is:
% 			\begin{equation}
% 				\vert I \vert \,\leq\, \lceil \log_2(m) \rceil \,+\, m \,\times\, \lceil \log_2(c_{max}) \rceil
% 			\end{equation}
% 			Where $c_{max}$ is the maximum integer value among $c_j$. We take the max because we want to encode all numbers the same way. \textit{m is exactly the number of c values, pay attention}.

% 		\subsection{Complexity Classes}
% 			When looking at an algorithm we usually want to calculate its time complexity: we're looking for a function that is an upper bound of the time needed to complete, dependently from the instance size. This function is usually expressed with asymptotic terms, with the big-O notation. This said, we can divide algorithms in classes of complexity, wheter they are asymptotically similar. The two classes of complexity of interest are
% 			\begin{itemize}
% 				\item Polynomial: $O(n^d)$
% 				\item Exponential: $O(2^n)$
% 			\end{itemize}
% 			where $n$,as usual, indicates the instance size.

% 			\subsubsection{Complexity Classes for \emph{Problems}}
% 				We can identify the \textit{problems} with the algorithm\footnote{the most efficent so far, generally} that solves them. In this way, we can distinguish between "easy" and "hard" problems. We classify problems in the two infamous classes
% 				\begin{itemize}
% 					\item $\mathcal{P}$ problems: the recognition\footnote{estabilishing if a property is verified, a true/false response.} problems that can be \textit{solved in polynomial time}
% 					\item $\mathcal{NP}$ problems, or \textit{nondeterministic polynomial} problems; these problems CAN'T be solved in polynomial time\footnote{so far}, but a solution can be verified in polynomial time.
% 				\end{itemize}
% 				Speaking in Turing machine terms, the problems in class $\mathcal{P}$ can be solved by a standard Turing machine, while the ones in $\mathcal{NP}$ by a nondeterministic one. N.B.: the nondeterministic Turing machine is just a Turing machine that \textit{verifies}\footnote{in polynomial time} a \textit{guessed} solution, ence the name "nondeterministic polynomial".\\
% 				Millennium prize problem: is $\mathcal{P} \,\equiv\, \mathcal{NP}$?

% 				\paragraph{Reductions}
% 					Reducing a problem to another means, more or less, solving a problem with another one. We say that $P_1$ \textit{reduces in polynomial time} to $P_2$ if we can solve $P_1$ using an algorithm that solves $P_2$ as subroutine, calling it a \textit{polynomial number of times}.\\
% 					It's straightforward to notice that if the algorithm used to solve $P_2$ has constant complexity, then $P_1$ will have a overall polynomial complexity. A reduction where the second algorithm is called only once is a \textit{transformation}.

% 				\paragraph{$\mathcal{NP}$-complete problems}
% 					A problem is said to be $\mathcal{NP}$-complete $\Leftrightarrow$
% 					\begin{itemize}
% 						\item it belongs to $\mathcal{NP}$ and
% 						\item every other problem in $\mathcal{NP}$ can be reduced to it in polynomial time
% 					\end{itemize}
% 					So these problems are
% 					\begin{enumerate}
% 						\item nondeterministic polynomial, so hard to \underline{solve} but simple to \underline{verify} (hard means above-polynomial, simple means polynomial or less)
% 						\item complete, because we can use them to represents a whole set of problems
% 					\end{enumerate}
				
% 				\paragraph{$\mathcal{NP}$-hard problems}
% 					A problem is $\mathcal{NP}$-hard when "every other problem in $\mathcal{NP}$ can be reduced to it in polynomial time". This problematic definition is cleared by this observation, I think: all recognition problems that are $\mathcal{NP}$-complete have their optimization version that's $\mathcal{NP}$-hard. 
	
% 	\clearpage
% 	\section{Integer Linear Programming}
% 		An ILP problem is a Linear Programming problem with the additional constraint $\underline{x} \in \mathbb{Z}^n$. Obiously we could add the integer constraints only to some varibales: in that case, the problem is said \textit{mixed} integer LP. An assumption we make (that does not influence the reasoning or the outcome) is that both \emph{A} and \emph{\underline{b}} are also composed of integers values.

% 		\subsection{Linear Relaxation and Relation with Linear Programming}
% 			If we remove the integer constraint froman ILP problem, weare left with a simple linear problem that shares some properties with the starting integer one:
% 			\begin{multicols}{2}
% 				\begin{equation}
% 					ILP: 
% 					\begin{cases}
% 						\max \underline{c}^tx \\
% 						Ax \leq \underline{b} \\
% 						x \geq \underline{0} \\
% 						x \in \mathbb{Z}^n
% 					\end{cases}
% 				\end{equation}	
% 				\break
% 				\begin{equation}
% 					LP:
% 					\begin{cases}
% 						\max \underline{c}^tx \\
% 						Ax \leq \underline{b} \\
% 						x \geq \underline{0}
% 					\end{cases}
% 				\end{equation}
% 			\end{multicols}
% 			The second one is called \textit{linear continuous relaxation} of the integer problem. For each ILP problem, we have that
% 			\begin{itemize}
% 				\item The objective function value of the optimal solution of the integer problem is always \textit{smaller in module} wrt the one of the linear relaxation
% 				\item The feasible region of the ILP problem is fully contained inside the one of the continuous relaxation: $X_{ILP} \subseteq X_{LP}$
% 			\end{itemize}
% 			\begin{minipage}{0.5\linewidth}
% 				These two properties are easily explained by the graphical representation of the feasible regions: it's easy to see how the "discreteness" of the ILP problem produces a "loss" of information (as the fractional part of solutions) and thus "reduces" the dimension of the feasible region.
% 			\end{minipage}
% 			\begin{minipage}{0.5\linewidth}
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/ILP1.png}
% 				\end{figure}
% 			\end{minipage}

% 		\subsection{Solving an ILP Problem}
% 			The first naive approach to solve an ILP problem could be to find the solutions of the associated linear relaxation, the round them to the "nearest" integer value that respects the boundaries. This method is not perfect because often the rounded solutions are \textit{infeasible} for the ILP problem or even \textit{useless}. The best application of this approach is when the objective function value is optimal for "large" numbers, so rounding does not affect in a significative way the actual OFV.\\
% 			Instead of focusing on the linear relaxation, we can directly solve the ILP problem exploiting its peculiarities:
% 			\begin{itemize}
% 				\item Implicit enumeration techniques explore all the feasible solutions by "traversing" them. In this category fall the "branch and bound" method but also dynamic programming techniques
% 				\item Cutting planes strategy aims to exploit the geometry of an integer problem, tuning the constraints of the feasible region in order to achieve an integer-bound feasible region to isolate easily a solution
% 				\item all sorts of heuristics can be used. These are not exact methods thogh.
% 			\end{itemize}

% 			\subsubsection{Branch and Bound Method}
% 				The idea behind branch and bound method is the classic "divide et impera" approach in algorithm design. In this particular problem, "divide" becomes "partition the feasible region into subregions" and "impera" is "resolve the problem locally in the subregion".

% 				\paragraph{Branching}
% 					Given the classic minimization problem $\{\min(c(x)) \,\vert\, x \in X\}$ we partition the feasible region as $X = \bigcup X_n$ with $X_i \bigcap X_j = \emptyset,\, i \neq j$. We can define $z_i = \min(c(x)) \,\vert\, x \in X_i$, so to have that the \textit{global} minimum is the minimum among all these \textit{local} $z_i$ minima. In an ILP problem, partitioning is really straightforward: we initially find a candidate solution for the linear relaxation: if such solution $\underline{x}$ is fractional, we divide the problem in the "greater or equal then" and "less or equal then" the integer approximation of the solution $\underline{x}$

% 				\paragraph{Bounding}
% 					For each subproblem defined by the branching phase, we can do three things:
% 					\begin{itemize}
% 						\item prove that the selected subregion of $X$ is empty
% 						\item prove that the objective function value won't decrease if we search in this subregion (so skipping the analysis for this particular subregion)
% 						\item determine a local optimal solution and update the value of the objective function found so far
% 					\end{itemize}
% 					this is a recursive method: we can also branch subregions and repeat the reasoning inside these subsubregions and so on.\\
% 					In an ILP, we just look for the solution of the linear relaxation of each subregion.

% 				\paragraph{B\&B Algorithm Summary}
% 					\begin{enumerate}
% 						\item Find the solution of the linear relaxation of the ILP considered
% 							\begin{itemize}
% 								\item if the solution is integer, return this value
% 								\item if it's fractional, go to step 2
% 							\end{itemize}
% 						\item $x_{LR}$ the solution at step 1, divide the feasible region in $x \leq \lfloor x_{LR} \rfloor$ and $x \geq \lfloor x_{LR} \rfloor + 1$ and return at step 1
% 					\end{enumerate}
% 					\begin{minipage}{0.5\linewidth}
% 						\begin{figure}[H]
% 							\centering
% 							\includegraphics[width = \textwidth]{./images/BaB1.png}
% 							\caption{The problem}
% 						\end{figure}
% 					\end{minipage}
% 					\begin{minipage}{0.5\linewidth}
% 						\begin{figure}[H]
% 							\centering
% 							\includegraphics[width = \textwidth]{./images/BaB2.png}
% 							\caption{Branch and bound decision tree with local solution values}
% 						\end{figure}
% 					\end{minipage}
% 					\vspace{0.5cm}
					
% 					The choice of "which subnode to evaluate next" can be done following the "deeper path" (so trying to fix the major number of nodes sequentially, the simplest version of the algorithm) or following the most promising nodes (the greedy approach). 

% 			\subsubsection{Cutting Plane Method and Gomory Fractional Cut}
% 				The B\&B method is a good method to find integer solutions of an ILP problems, but has the common problem of recursive tree-based algorithms: overall temporal complexity, difficult to evaluate single nodes etc. The cutting planes method aims, as already said, to exploit the peculiarity of the "integer geometry" of the ILP feasible region.\\
% 				The idea is: the polyhedron described by the feasible region could be "readjusted" in order to costraint \textit{exactly} the integer solutions, and not also the fractional part of the relaxation. It's easily explained by an example: if the "rightmost" constraint of our two-dimensional feasible region is $x \leq 2.15$, we can readjust the constrainf to $x \leq 2$ without losing solutions of the ILP problem. Also, it would be simpler to find an exact optimal solution along this constraint. If we repeat the reasoning for all rows of matrix $A$, we could "shrink" the feasible region down to have just the integer solutions on the vertices. We define an \textit{ideal formulation} of the feasible region as the one that describes the \textit{convex hull} of $X_{ILP}$, so the \textit{smallest convex set that encloses} $X_{ILP}$. REMEMBER: $X_{ILP}$ is a \underline{discrete} set, while the hull is a continuous polyhedra. This definition is useful when paired when the theorem:\\
% 				for any feasible region $X_{ILP}$ of an integer linear programming problem, there exists an ideal formulation involving a finite number of linear constraints.\\
% 				Usually, the number of constraints of the ideal formulation is exponential wrt the original formulation: additional constraints are usually needed to "encapsule" the original discrete set of feasible solutions.

% 				\paragraph{Gomory's Cutting Plane Method}
% 					The idea is: we don't need to calculate the \textit{entire} convex hull, we just focus on the optimal solution "neighbourhood" and progressively "cut away" fractional parts until we reach an integer optimal solution. We define a Gomory cut as:
% 					\begin{equation}
% 						\sum_{j:x_j \in N} ((a_{rj} - \lfloor a_{rj} \rfloor) x_j) \geq (b_r - \lfloor b_r \rfloor) 
% 					\end{equation}
% 					Where $r$ represents a row in the matrix $A$, $N$ is the set of non basic variables, $a$ and $b$ are respectively the left hand side and right hand side coefficients of that equation in the system. The Gomory cut is a \textbf{cutting plane}: it is violated by the optimal solution of the linear relaxation, but it is satisfied by all the integer solutions of the original problem.

			


% 	\clearpage
% 	\section{Examples}
% 		This section collects all the example, following the course contents schema
		
% 		\subsection{Graphs}
% 			\paragraph{Cuts and incompatibility relations}
% 				Problem to model: assigning tasks to the right-qualified engineers. We create a set N of node composed of $N_t$ the tasks and $N_e$ the engineers. We then observe the cuts between the two subsets (obviously, $N_t = N \setminus N_e$) to see if we have enough engineers for the tasks. We will return on this example later.

% 			\paragraph{Directionality and precedence}
% 				We can use the directionality of graphs to model precedence (among tasks, activities, phases...). Two alternative representations are usually used:
% 				\begin{enumerate}
% 					\item Nodes as activities, arcs as precedence relations
% 					\item Arcs as activities, nodes as checkpoints
% 				\end{enumerate}
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/Precedence.png}
% 				\end{figure}

% 			\paragraph{Minimum spanning trees and optimal message passing}
% 				Wecan model a network of peers as a weighted graph where each node is a peer and each edge has associated (as the weight) the probability of interception of a message along that channel. The problem of finding a broadcasting tree with the minimum interception probability is a minimum spanning tree problem. We call $p_{i\, j}$ the probability of interception between nodes i and j, and the objective function of the problem is simply
% 				\begin{equation}
% 					max\, \prod_{i, j \in T} (1 - p_{i\, j})
% 				\end{equation}

% 			\paragraph{O($n^2$) Prim's algorithm formulation}
% 				This formulation of the algorithm uses as data structure a vector that's long as the number of nodes. Each entry in the vector is numbered as the associated node, so that this vector represents \textit{the predecessors} at the end of the algorithm. At startup, it will be filled with the ID of the starting node. At each iteriation, the vector is updated where a path is found that decreases the cost of a node (if node 3 is reachable through node 2 with a decreasing-cost edge, the enntry in position 3 will be updated to 2).
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/Prim.png}
% 				\end{figure}

% 			\paragraph{DAGs and Gantt charts}
% 				Given the output of the Critical Path Method, we can build a visual chart that represents the actual parallelism and slack of each activity in a more intuitive way. Example:
% 				\begin{figure}[H]
% 					\centering
% 					\includegraphics[width = \textwidth]{./images/Gantt.png}
% 				\end{figure}

% 			\paragraph{Network flow and matching problems}
% 				The already discussed task-engineers problem can be seen as a network flow problem, where we're trying to maximize the flow between the first half (the engineers) and the second half of the nodes (the tasks). Note that the graph must be \textit{bipartite}.\\
% 				The idea is to add a fake initial and final node to have a connected graph (the weights of the additional edges must be all equal) and then finde the maximum possible flow through the network: this will highlight the best combination of assignements for the engineers.


% 		\subsection{Integer Linear Programming}
% 			\paragraph{Knapsack problem}
% 				We have to determine a subset of the carriable objects that fit into the knapsack and maximize the value.
% 				\begin{equation}
% 					\begin{cases}
% 						n \text{ objects} \\
% 						p_j \text{ profit for object } j \\
% 						v_j \text{ volume for object } j \\
% 						b \text{ maximum sack capacity}
% 					\end{cases}
% 				\end{equation}
% 				we define binary variable $x_j$ that indicates if we chose to pick object $j$. The problem has a very easy to understand objective function and feasible region:
% 				\begin{equation}
% 					\begin{cases}
% 						\max(\sum_{j=1}^n p_j x_j) \text{ maximize profit}\\
% 						\sum_{j=1}^n v_j x_j \leq b \text{ maximum volume constraint}\\
% 						x_j \in \{0, 1\} \forall j \text{ decision variable constraint}
% 					\end{cases}
% 				\end{equation}

% 			\paragraph{Transportation problem}
% 				Classic offer-demand-constraint problem, characterized by:
% 				\begin{equation}
% 					\begin{cases}
% 						m \text{ production plants} \\
% 						n \text{ clients} \\
% 						c_{ij} \text{ transportation cost from } i \text{ to } j \\
% 						p_i \text{ production capacity for plant } i \\
% 						d_j \text{ demand of client } j \\
% 						q_{ij} \text{ maximum amount that can be transported on that road}
% 					\end{cases}
% 				\end{equation}
% 				Again a very intuitive problem to solve: we must minimize the total transportation cost satisfying the demands and without exceeding the transportation limit of roads. We use decision variable $x_{ij}$ to describe the amount of product transported from plant to clinet.
% 				\begin{equation}
% 					\begin{cases}
% 						\min(\sum_J ( \sum_I (c_{ij} \times x_{ij}))) \\
% 						\sum_I (x_{ij}) \geq d_j \\
% 						\sum_J (x_{ij}) \leq p_i \\
% 						0 \leq x_{ij} \leq q_{ij} \forall (i, j)
% 					\end{cases}
% 				\end{equation}


% \end{document}
