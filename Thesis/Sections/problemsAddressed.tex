\begin{enumerate}
	\item database choice and API modification
	\item queue monitoring
	\item resizable backend containers
	\item cloud provider integration
\end{enumerate}

\subsection{Scalability}
  The MapNCloud system was designed to ensure a detachment between the "business logic" backend and the "computational" backend, in order to ensure that that the full application does not get stuck on a request by offloading longer tasks to a different independent module. This module (\ref{ssse:originalcomputationlayer}) is the main bottleneck of the system, and though the one I focused on to improve its overall scalability.\\
  The first approach considered was to use an automated orchestration software as Kubernetes (\ref{sse:kubernetes}) or a task scheduler as SLURM (\ref{sse:slurm}) to manage the mapping of tasks on available computational units; as explained in the respective sections, these approaches were not viable, due to either an high infrastructure cost or a poor cloud compatibility. The system was required to be able to handle different traffic paces, with different requests weights, being at the same time cost effective\footnote{which translates, in cloud environments, to deallocate unused resources}.\\
  The target solution needed to dynamically spawn workers, each one tailored to the actual traffic detected or to a forecast on traffic (based on past data or statically, using time of day as index for example). Each worker had to be spawned with a different set of resources, to better handle the traffic. As an example, if a great number of requests suddenly arrive all together, each one with an high number of input files, some resourceful workers needed to be spawned; instead, if the number of unacknowledged tasks grows steadily but slowly, a smaller\footnote{smaller in the number or size of resources allocated: number of CPUs, amount of RAM and optional GPUs} worker unit can be created to handle the accumulating tasks. This kind of reasoning \textit{must} be applied also in the other direction: especially in the cloud, systems that allocates a significant number of hardware resources are very expensive; if heavy workers (i.e. some modules with high CPU or GPU count) are idling, or the system can satisfy the incoming traffic without them, in order to be cost effective the system should be able to identify the unnecessary workers and kill them. The final design uses an automation tool provided by the cloud vendor to expose a REST interface that allows an alerting module (built in the queuing system) to notify congestion situations, and to also shape the worker spawned in terms of CPU count, GPU count and memory available.

\subsection{Cloud Provider Integration}
  The choice of the cloud provider was discussed in the first steps of the project, and after some initial performance tests Azure by Microsoft has been chosen. These simple tests were conducted to see if there was a significant difference in performance between the two vendors: the results are available in the dedicated appendix. In the choice of the cloud vendor other factors have been relevant, as the ease of integration between internal services, general support provided by the vendor itself, migration problems and developer familiarity with the tools.
